# Focal Frequency Loss

![teaser](resources/teaser.jpg)

This repository will provide the official code for the following paper:

**Focal Frequency Loss for Image Reconstruction and Synthesis**<br>
[Liming Jiang](https://liming-jiang.com/), [Bo Dai](http://daibo.info/), [Wayne Wu](https://wywu.github.io/) and [Chen Change Loy](http://personal.ie.cuhk.edu.hk/~ccloy/)<br>
arXiv preprint, 2020.<br>
[**Paper**](https://arxiv.org/abs/2012.12821)
> **Abstract:** *Image reconstruction and synthesis have witnessed remarkable progress thanks to the development of generative models. Nonetheless, gaps could still exist between the real and generated images, especially in the frequency domain. In this study, we show that narrowing gaps in the frequency domain can ameliorate image reconstruction and synthesis quality further. We propose a novel focal frequency loss, which allows a model to adaptively focus on frequency components that are hard to synthesize by down-weighting the easy ones. This objective function is complementary to existing spatial losses, offering great impedance against the loss of important frequency information due to the inherent bias of neural networks. We demonstrate the versatility and effectiveness of focal frequency loss to improve popular models, such as VAE, pix2pix, and SPADE, in both perceptual quality and quantitative performance. We further show its potential on StyleGAN2.*

## Updates

- [12/2020] The [paper](https://arxiv.org/abs/2012.12821) of Focal Frequency Loss is released on arXiv.

## Code

The code will be made publicly available. Please stay tuned.

## Citation

If you find this work useful for your research, please cite our paper:

```
@article{jiang2020focal,
  title={Focal Frequency Loss for Image Reconstruction and Synthesis},
  author={Jiang, Liming and Dai, Bo and Wu, Wayne and Loy, Chen Change},
  journal={arXiv preprint},
  volume={arXiv:2012.12821},
  year={2020}
}
```

## License

Copyright (c) 2020
